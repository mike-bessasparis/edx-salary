---
title: "Data Science: Capstone Project Report"
output:
  pdf_document: 
    fig_height: 4
  html_notebook: default
---

```{r echo = FALSE, message = FALSE}

knitr::knit_hooks$set(inline = function(x) {
  prettyNum(x, big.mark=",")
})


```


```{r echo = FALSE, message = FALSE, results = 'hide'}

library(tidyverse)
library(gridExtra)
library(ggthemes)

c_names <- c("age", "workclass", "fnlwgt", "education", "education-num",
           "marital-status", "occupation", "relationship", "race", "sex", "capital-gain",
           "capital-loss", "hours-per-week", "native-country", "salary")

mjb <- read.csv(file = "./data/adult.data", header = FALSE, col.names = c_names)

# remove leading space in salary factor
mjb <- mjb %>% 
        mutate(salary = factor(if_else(salary == " <=50K", "lte50K", "gt50K")))

rm(c_names)

```
## Executive Summary
The goal of this project is to predict the salary given some U.S. Census data. I selected this project in order to attempt to match the prediction accuracy cited in a 1996 paper, Improving the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid.[^1]  A summary of the results is found in the file adult.names.

This is a classification problem.  We don't predict the exact salary.  Rather, we predict which class (>$50,000 or <=$50,000) that each individual belongs.

The key steps are importing the data set, cleaning the data set, creating and tuning a model, and evaluating the model against a validation data set. In the original data set work the authors listed the top 3 performing models: NBTree, C4.5, and Naive-Bayes.  I choose a xgbTree, Naive-Bayes, and C5.  I split the data set into a train_set and a test_set in order to avoid making any modelling decisions with the validation set. Then model parameters are tuned and evaluated against the test_set.  Finally, the models make predictions for the validation set.  

## Methods
The data required minimal cleaning.  I removed the observations with unknown values because the original researchers did.

First I build the data sets.  The data files consist of two files: adult.data and adult.validation.  I split the adult.data set into a train and test set.  I train three models on the training data.  When the models are built I use these models to predict the whether the salary of individuals in the validation set are greater than or less than or equal to $50,000.

## Results
The original project used accuracy as evaluation function and so did I.  One model, C5, out-performed the original models.  

| Original Model | Accuracy | My Model | Accuracy |
|-------|----------|--------|----------|
| NBTree | 0.8590 | xgbTree | 0.8509 |
| C4.5  | 0.8446 | C5 | 0.8465 |
| Naive-Bayes | 0.8388 | Naive-Bayes | 0.8208 |


## Conclusion
I've learned quite a bit during this course.  I can build models that predict almost as well as others with much more experience than me.  Of course, I've only learned how to use the tools that others have built.  That counts as success for my first data science course.

A detailed exploratory data analysis follows.

\newpage

## Basic Overview of Data
I obtained the data from https://archive.ics.uci.edu/ml/datasets/census+income.  This data was oringally extracted from the 1994 census bureau database.  The data was originally donated to the UCI by Ronny Kohavi and Barry Becker, Data Mining and Visualization Silicon Graphics, e-mail: ronnyk@sgi.com for questions.  Extraction was done by Barry Becker from the 1994 Census database.

[^1]: Kohavi, Ron, Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid, Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996. http://www.aaai.org/Papers/KDD/1996/KDD96-033.pdf

***
### Structure
```{r echo=FALSE}

str(mjb)

```

Distribution of the response variable.  We see this is a fairly unbalanced data set.

```{r echo=FALSE}

mjb %>% 
        group_by(salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = count, fill = salary)) +
                geom_col(position = "dodge", show.legend = FALSE) +
                geom_text(aes(label = round(pct, 2),
                          y = count * 0.5), size = 4, colour = "white") +
                labs(title = "Salary") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")



```

\newpage
***
## Univariate Distribution
### Histograms
Let's take a look at the numeric variables.  If we use AGE as a predictor we would likely center and normalize.

```{r echo=FALSE}

h1 <- ggplot(mjb, aes(x = age)) + geom_histogram(binwidth = 1, fill = "#1380A1") + theme_economist_white()
h2 <- ggplot(mjb, aes(x = education.num)) + geom_histogram(binwidth = 1, fill = "#1380A1") + theme_economist_white()
h3 <- ggplot(mjb, aes(x = capital.gain)) + geom_histogram(fill = "#1380A1") + theme_economist_white()
h4 <- ggplot(mjb, aes(x = capital.loss)) + geom_histogram(fill = "#1380A1") + theme_economist_white()
h5 <- ggplot(mjb, aes(x = hours.per.week)) + geom_histogram(binwidth = 5, fill = "#1380A1") + theme_economist_white()

grid.arrange(h1, h2, h3, h4, h5)

```

Next we look at the categorical data.

```{r echo=FALSE}

h1 <- ggplot(mjb, aes(x = workclass)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white()
h2 <- ggplot(mjb, aes(x = education)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white() + theme(axis.text.y = element_text(size = 8))
h3 <- ggplot(mjb, aes(x = marital.status)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white()
h4 <- ggplot(mjb, aes(x = occupation)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white() + theme(axis.text.y = element_text(size = 8))
h5 <- ggplot(mjb, aes(x = relationship)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white()
h6 <- ggplot(mjb, aes(x = race)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white()
h7 <- ggplot(mjb, aes(x = sex)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white()
h8 <- ggplot(mjb, aes(x = native.country)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white() + theme(axis.text.y = element_text(size = 8))
h9 <- ggplot(mjb, aes(x = salary)) + geom_bar(fill = "#1380A1") + coord_flip() + theme_economist_white() 


grid.arrange(h1, h2, h3, h4)
grid.arrange(h5, h6, h7, h9) 
grid.arrange(h8)

rm(h1, h2, h3, h4, h5, h6, h7, h8, h9)

```

We remove those observations with an unknown value.  We could have imputed the values, perhaps considering the distribution of known values.  We did not because the original authors removed the data.

```{r echo=FALSE}

mjb <- mjb %>% 
        filter(workclass != " ?", 
               occupation != " ?",
               native.country != " ?")

```

\newpage
***
## Bivariate
### Response and predictor relationships
#### Continuous Predictors
We see that age can predict low salary for young adults.

```{r echo=FALSE}

p2 <- mjb %>% 
        ggplot(aes(x = salary, y = age, fill = salary)) +
                geom_boxplot() +
                coord_flip() +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


p3 <- mjb %>% 
        ggplot(aes(x = age, fill = salary)) +
                geom_density(alpha = 0.80) +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme_economist_white()

grid.arrange(p2, p3)

# mjb %>% group_by(salary) %>% 
#         summarise(quant = pnorm(20, mean(age), sd(age))) 
        
rm(p2, p3)

```


```{r echo=FALSE}

mjb %>% 
        filter(capital.gain != 0) %>% 
        ggplot(aes(x = salary, y = capital.gain, fill = salary)) +
                geom_boxplot() +
                coord_flip() +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")

```

It appears that Capital Gain can also help predict salary.  Capital Loss is not as helpful.

```{r echo=FALSE}

mjb %>% 
        filter(capital.loss != 0) %>% 
        ggplot(aes(x = salary, y = capital.loss, fill = salary)) +
                geom_boxplot() +
                coord_flip() +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```

```{r echo=FALSE}

mjb %>% 
        ggplot(aes(x = salary, y = hours.per.week, fill = salary)) +
                geom_boxplot() +
                coord_flip() +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```

\newpage
***
#### Categorical Predictors
Looking at the categorical predictors we see several that appear useful. We will want to verify our intuition by considering independence using a Chi-Square test.

```{r echo=FALSE}

mjb %>% 
        group_by(workclass, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge", show.legend = FALSE) +
                facet_wrap(~ workclass) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Workclass", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```
```{r echo=FALSE}

mjb %>% group_by(marital.status, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ marital.status) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Marital Status", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")

```

```{r echo=FALSE}

mjb %>% 
        group_by(education, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ education) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Education", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")

```


```{r echo=FALSE}

mjb %>% 
        group_by(occupation, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ occupation) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Occupation", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```


```{r echo=FALSE}

mjb %>% 
        group_by(relationship, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ relationship) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Relationship", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```


```{r echo=FALSE}

mjb %>% 
        group_by(race, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ race) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Race", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```

```{r echo=FALSE}

mjb %>% 
        group_by(sex, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                geom_col(position = "dodge") +
                facet_wrap(~ sex) +
                scale_y_continuous(breaks = NULL) +
                labs(title = "Sex", subtitle = "Percentages") +
                theme_economist_white() +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```

```{r echo=FALSE}

mjb %>% 
        group_by(native.country, salary) %>% 
        summarise(count = n()) %>%
        mutate(pct = count/sum(count)) %>%
        ggplot(aes(x = salary, y = pct, fill = salary)) +
                scale_y_continuous(breaks = NULL) +
                geom_col(position = "dodge") +
                facet_wrap(~ native.country) +
                labs(title = "Native Country", subtitle = "Percentages") +
                theme_economist_white() +
                theme(strip.text.x = element_text(size = 8)) +
                scale_fill_manual(values = c("#990000", "#1380A1")) +
                theme(legend.position="none")


```

\newpage
We apply a Chi-Square test for independence for each variable against `salary`.  We calculate the Cramer's V value for each variable against `salary`.  We see that `workclass`, `occupation`, and `native.country` do not get a valid Cramer's V value since they fail to produce a correct Chi-Square test.

Those variables that are important to predicting salary are `education`, `marital.status`, `relationship`, and `sex`.  If we run into model performance constraints we can reduce the categorical predictors to these.  

```{r echo=FALSE, results=FALSE, warning=FALSE, message=FALSE}

library(DescTools)
library(ggplot2)

print("Workclass")
dt <- table(mjb[, c("salary", "workclass")])
chi <- chisq.test(dt)

print("occupation")
dt <- table(mjb[, c("salary", "occupation")])
chi <- chisq.test(dt)

print("relationship")
dt <- table(mjb[, c("salary", "relationship")])
chisq.test(dt)

print("race")
dt <- table(mjb[, c("salary", "race")])
chisq.test(dt)

print("sex")
dt <- table(mjb[, c("salary", "sex")])
chisq.test(dt)

print("native.country")
dt <- table(mjb[, c("salary", "native.country")])
chisq.test(dt)

df <- mjb %>% select_if(is.factor)

m <- PairApply(df, FUN = CramerV)

corrplot::corrplot(m, type = "lower", is.corr = FALSE, method = "shade",
                   addCoef.col = "black", diag = FALSE, cl.pos = "n", tl.cex = 1, tl.srt = 30)


```







