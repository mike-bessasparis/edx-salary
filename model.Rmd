---
title: "R Notebook"
output: html_notebook
---

Load the training data. I also remove the variable 'fnlwgt' since I don't understand what it represents.  See the description in the file 'adult.names'.
```{r}

library(tidyverse)
library(caret)
library(doParallel)

c_names <- c("age", "workclass", "fnlwgt", "education", "education.num",
           "marital.status", "occupation", "relationship", "race", "sex", "capital.gain",
           "capital.loss", "hours.per.week", "native.country", "salary")

mjb <- read.csv(file = "./data/adult.data", header = FALSE, col.names = c_names)

# remove leading space in salary factor
mjb <- mjb %>% 
        mutate(salary = factor(if_else(salary == " <=50K", "lte50K", "gt50K")))

# remove unknowns
mjb <- mjb %>% 
        filter(workclass != " ?", 
               occupation != " ?",
               native.country != " ?")

mjb <- mjb %>% dplyr::select(-fnlwgt)

```


Load the validation set making the same changes as the training data.
```{r}

validation_set <- read.csv(file = "./data/adult.validation", header = FALSE, skip = 1, col.names = c_names)

# remove leading space in salary factor
# remove the trailing "." found only in the adult.validation file.
# rename factor to not include "<=" or ">"
validation_set <- validation_set %>% 
        mutate(salary = factor(if_else(salary == " <=50K.", "lte50K", "gt50K")))

# remove unknowns
validation_set <- validation_set %>% 
        filter(workclass != " ?", 
               occupation != " ?",
               native.country != " ?")

validation_set <- validation_set %>% dplyr::select(-fnlwgt)

rm(c_names)

```


Create train and test sets from the training data.
```{r}

# test_set set will be 10% of data
set.seed(1)
test_index <- createDataPartition(y = mjb$salary, times = 1, p = 0.1, list = FALSE)
train_set <- mjb[-test_index,]
test_set <- mjb[test_index,]


```


Random Forest to validate important variables from EDA.
```{r}

# setup for parallel processing 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
registerDoParallel(cl)

start_rf <- Sys.time()

control <- trainControl(method = "cv", number = 10, p = .9)

fit_rf <- train(salary ~ .,
             method = "ranger",
             trControl = control,
             data = train_set,
             importance = 'impurity')

varImp(fit_rf)

stopCluster(cl)


```

Naive-Bayes model.
```{r}

# setup for parallel processing 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
registerDoParallel(cl)

start_nb <- Sys.time()

control <- trainControl(method = "cv", number = 10, p = .9)

fit_nb <- train(salary ~ age + capital.gain + hours.per.week + marital.status + relationship + education.num + race + sex,
                  method = "nb", 
                  trControl = control,
                  data = train_set)

preds <- predict(fit_nb, test_set)

dat <- data.frame(obs = test_set$salary, pred = preds)

confusionMatrix(dat$pred, dat$obs)

stopCluster(cl)


```

Extreme Gradient Boosting Tree model.  The original work used a NBTree model - a tree based model using Naive-Bayes for the leaves.  I don't have access to that model so I used XGBTree instead.
```{r}

# setup for parallel processing 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
registerDoParallel(cl)

start_xgb <- Sys.time()

control <- trainControl(method = "cv", number = 10, p = .9)

grid <- expand.grid(nrounds = 379, max_depth = 9, eta = 0.1601659, gamma = 6.955552, 
                    colsample_bytree = 0.5640455, min_child_weight = 0, subsample = 0.9083501)

fit_xgb <- train(salary ~ age + capital.gain + hours.per.week + marital.status + relationship + education.num + race + sex,  
                method = "xgbTree", 
                trControl = control,
#                tuneGrid = grid,
                data = train_set,
                allowParallel = TRUE)

preds <- predict(fit_xgb, test_set)

dat <- data.frame(obs = test_set$salary, pred = preds)

confusionMatrix(dat$pred, dat$obs)

stopCluster(cl)

```



C5 model.  The original work used the C4.5 model.  I used the improved model - C5.  C5 is faster and uses much less memory than C4.5.  It also includes boosting.
```{r}

# setup for parallel processing 
cores <- detectCores()
cl <- makePSOCKcluster(cores-1)
registerDoParallel(cl)

start_c5 <- Sys.time()

control <- trainControl(method = "cv", number = 10, p = .9)

fit_c5 <- train(salary ~ age + capital.gain + hours.per.week + marital.status + relationship + education.num + race + sex,  
             method = "C5.0", 
             trControl = control,
             data = train_set)

preds <- predict(fit_c5, test_set)

dat <- data.frame(obs = test_set$salary, pred = preds)

confusionMatrix(dat$pred, dat$obs)

stopCluster(cl)

```


```{r}

# final predictions and accuracy using the validation set


preds <- predict(fit_nb, validation_set)
dat <- data.frame(obs = validation_set$salary, pred = preds)
cm <- confusionMatrix(dat$pred, dat$obs)
results <- data_frame(method = "Naive-Bayes", accuracy = cm$overall["Accuracy"])

preds <- predict(fit_xgb, validation_set)
dat <- data.frame(obs = validation_set$salary, pred = preds)
cm <- confusionMatrix(dat$pred, dat$obs)
results <- bind_rows(results, 
                     data_frame(method = "xgbTree", accuracy = cm$overall["Accuracy"]))

@results <- bind_rows(results, 
                     data_frame(method = "C5", accuracy = cm$overall["Accuracy"]))

results

```


